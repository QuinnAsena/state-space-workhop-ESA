### Install packages {.unlisted .hidden}

```{r}
source("./install.r")
```


## Some prep

Welcome to our workshop on how to use the `multinomialTS` package. First things first, let's make sure you have all the necessary packages by rendering this document. This may take a few minutes, so while that is running, I'm going to go through a few slides. If this document fails to render, something has gone wrong and one of our helpers/minions will help you sort it out.

## Introduction

This workshop will take you through how to fit the `multinomialTS` model and interpret the results. This model has been designed specifically to work with a multinomially distributed response variable, that is, data from a finite number individuals (e.g., 300 counts) from a sample, from multiple possible classes (e.g., different taxon). We will be using a palaeoecological dataset as an example, where a number of fossil pollen grains are counted (usuall 300-400) and identified per slice of sediment. From these data, we know the relative abundance of taxa in the sample, but not the absolute abundance of each species on the landscape. Knowing only the relative abundances creates interdependencies among the taxa as we can only generate estimates $n-1$ taxa. Put simply, if we have 3 species and 60% of the counts belong to species 1, and 30% of the counts belong to species 2, then the remaining 10% must therefore belong to species 3.

The covariate data can be of mixed mixed types, such as binary events or charcoal accumulation rates.

The data are divided into two, the multinomially distributed ecological state variables (e.g., pollen counts), and the environmental covariates, which may be of mixed types (e.g., binary events or charcoal accumulation rates). Analysing these data with `multinomialTS` can be split into two decision streams (@fig-flow):

1. Handling the state variables $Y$
2. Handling the covariates $X$
   
![The process of handling the state variables, the covariates, and fitting the model](./images/flowchart1.png){#fig-flow}


We will go through an example of how to do this in detail now.


## Data from Story Lake

Let's get some data. We are going to use data that are the pollen counts from Story Lake, IN [@schlenker2024], these data will be our state variables $Y$. The data also contain charcoal counts, which will be our environmental covariate $X$. The data are stored in the [Neotoma database](https://apps.neotomadb.org/explorer/).

The following (folded) code block will download the data directly from the Neotoma API, do basic harmonisation of taxa, and some data wrangling to give us a wide dataset of state variables ($Y$) and environmental covariates ($X$). For this session, the data are already wangled and saved to the data directory, so the code block is set not to run `#| eval: false`. We are not going to delve into the details of taxonomic resolution in this workshop, for details on using the `neotoma2` package check out the neotoma [GitHub](https://github.com/NeotomaDB/Current_Workshop/tree/main).


```{r}
#| code-fold: true
#| eval: false

# Find site in neotoma
story_site <- get_sites(sitename = "Story%")

# Download the data from the site
# Downloading the data only needs to be done once
story_samples <- story_site %>%  
    get_downloads() %>% 
    samples()

# Save the raw data. Always keep an untouched copy of the raw data.
saveRDS(story_samples, "./data/story_samples.rds")

# Read-in the raw samples, this avoids repeatedly downloading from the API
story_samples <- readRDS("./data/story_samples.rds")

# Some harmonisation of species, see Schlenker et al., 2024 in refs for details
# First filter the data for pollen sampels, then harmonise a few species to the genus-level
story_pollen_long <- story_samples %>%
  filter(elementtype == "pollen",
         units == "NISP") %>% 
  mutate(variablename = replace(variablename, stringr::str_detect(variablename, "Pinus*"), "Pinus"),
         variablename = replace(variablename, stringr::str_detect(variablename, "Picea*"), "Picea"),
         variablename = replace(variablename, stringr::str_detect(variablename, "Acer*"), "Acer"),
         variablename = replace(variablename, stringr::str_detect(variablename, "Juglans*"), "Juglans"),
         variablename = replace(variablename, stringr::str_detect(variablename, "Fraxinus*"), "Fraxinus")
  ) %>% 
  group_by(siteid, sitename,
           sampleid, variablename, units, age,
           agetype, depth, datasetid,
           long, lat) %>%
  summarise(value = sum(value), .groups='keep')

# Check names for any variables like spike, trace, Lycopodium, Microspheres...
unique(story_pollen_long$variablename)

saveRDS(story_pollen_long, "./data/story_pollen_long.rds")


# Now let's turn to the charcoal
# first filter data for charcoal counts
story_char <- story_samples %>%
  filter(elementtype == ">125 Âµm")

# The following code pivots the data to a wide dataframe
# then does a very crude calculation to convert charcoal counts to charcoal accumulation rate
# for the purposes of this workshop the crude calculation will do
story_char_wide <- story_char %>% 
  pivot_wider(id_cols = c(age, depth), names_from = variablename, values_from = value) %>% 
  mutate(lag_depth = abs(lag(depth) - depth),
  lag_age = abs(lag(age) - age),
  acc_rate = lag_depth / lag_age,
  char_acc = acc_rate * Charcoal) %>% 
  mutate(across(everything(), ~ ifelse(row_number() == 1 & is.na(.), 0, .))) %>% 
  select(age, depth, char_acc)

# Let's save the wrangled data
saveRDS(story_char_wide, "./data/story_char_wide.rds")

```

Since the folded code block above has already been run, and the data saved to the data directory, we can read in the wrangled data directly. Currently the both the $Y$ and $X$ data contain age and depth columns. These columns will be excluded later but I like to keep some form of common ID column at this stage.


```{r}
# Read-in the wrangled data 
# Read in our X variables
story_char_wide <- readRDS("./data/story_char_wide.rds")
# Read in the long data for plotting
story_pollen_long <- readRDS("./data/story_pollen_long.rds")
```

## Handling the state variables $Y$

 Let's start handling our state variables $Y$. It is not recommended to attempt to estimate coefficients for every species in the data, so we are going to look at key taxa and groups. Let's do some quick and dirty visualisation of the data, by plotting the top five most abundant species by their counts. It can be useful to explore more than the top five species by their counts of course, for example there may be key species ecologically that occur in low abundances. We recommend methods like cluster anayses and topic modelling as preliminary, or complimentary, analyses but we will not cover those methods in this workshop.

```{r}
#| code-fold: true
#| warning: false
#| label: fig-topfive
#| fig-cap: "Pollen counts of the five species with the highst counts in the record. All other species are grouped in the 'other' species group"

# There are multiple ways of coding the data-wrangling
# I'm sticking with tidyverse for consistency but
# I sometimes BASE is more efficient!
story_pollen_top5_count <- story_pollen_long %>%
  group_by(variablename) %>%
  mutate(total_count = sum(value)) %>%
  slice(1) %>% 
  ungroup() %>%
  select(variablename, total_count) %>% 
  arrange(desc(total_count)) %>% 
  slice(1:5)

story_top5 <- story_pollen_long %>% 
  filter(variablename %in% c(story_pollen_top5_count$variablename))

story_other <- story_pollen_long %>%
  filter(!variablename %in% c(story_pollen_top5_count$variablename)) %>%
  mutate(variablename = "other") %>%
  group_by(variablename, age) %>%
  summarise(value = sum(value))

story_top5_res <- bind_rows(story_top5, story_other) %>%
  mutate(variablename = factor(variablename, levels =  c(
    "other", rev(story_pollen_top5_count$variablename)
  )))

ggplot(story_top5_res, aes(x = age, y = value)) +
  geom_area(colour = "grey90") +
  geom_col() +
  scale_x_reverse(breaks = scales::breaks_pretty(n = 6)) +
  coord_flip() +
  # ylim(0, 0.5) +
  labs(y = "Pollen counts", x = "Time (ybp)") +
  facet_wrap(~variablename,
             nrow = 1) +
  theme_minimal() +
  theme(
    text = element_text(size = 10),
  )
```

From @fig-topfive, we can see that oak (_Quercus_) American Beech (_Fagus grandifolia_) and elm (_Ulmus_) occur in high counts. Ash (_Fraxinus_) and hickory (_Carya_), also occur throughout the record but at lower counts. The large peak at the most recent part of the record, in the grouped 'other' species, is from an increase in ragweed (_Ambrosia_) after Euro-American land clearance [@schlenker2024]. This gives us a starting point for deciding which are our target taxa. Story lake data are dominated by three taxa (oak, beech, and elm), so we are going to use these as out target taxa. Addtionally, from @schlenker2024, there are notable hardwood taxa: hickory (_Carya_), hornbeam (_Ostrya/Carpinus_), ash (_Fraxinus_), walnut (_Juglans_), sycamore (_Platanus_) and maple (_Acer_). With the ecological knowledge of the hardwood taxa, I am also creating a 'hardwood' group.

```{r}
hardwood_names <- c("Carya", "Ostrya/Carpinus", "Acer", "Juglans", "Fraxinus")
target_taxa <- c("Ulmus", "Fagus grandifolia", "Quercus")

# Create a group of hardwood taxa by summing counts
story_pollen_long_hardwood <- story_pollen_long %>%
  filter(variablename %in% hardwood_names) %>% 
  mutate(variablename = "hardwood") %>% 
  group_by(siteid, sitename,
           sampleid, variablename, units, age,
           agetype, depth, datasetid,
           long, lat) %>%
  summarise(value = sum(value), .groups='keep')

# Filter for target taxa
story_pollen_long_targets <- story_pollen_long %>%
    filter(variablename %in% target_taxa)

# Sum everything else into our reference group
story_other_taxa <- story_pollen_long %>%
  filter(!variablename %in% c(hardwood_names, target_taxa)) %>%
  mutate(variablename = "other") %>% 
  group_by(siteid, sitename,
           sampleid, variablename, units, age,
           agetype, depth, datasetid,
           long, lat) %>%
  summarise(value = sum(value), .groups='keep')

state_variables <- bind_rows(story_other_taxa, story_pollen_long_hardwood, story_pollen_long_targets) %>% 
  mutate(variablename = factor(variablename, levels =  c("other", "hardwood", target_taxa)),
         value = replace_na(value, 0))

```

Let's check-out our new grouping:

```{r}
#| code-fold: true
#| label: fig-targettaxa
#| fig-cap: "Pollen counts of the most common taxa with an additional group of 'hardwood' taxa."

ggplot(state_variables, aes(x = age, y = value)) +
  geom_area(colour = "grey90") +
  geom_col() +
  scale_x_reverse(breaks = scales::breaks_pretty(n = 6)) +
  coord_flip() +
  # ylim(0, 0.5) +
  labs(y = "Pollen counts", x = "Time (ybp)") +
  facet_wrap(~variablename,
             nrow = 1) +
  theme_minimal() +
  theme(
    text = element_text(size = 10),
  )
```


The data are `r round(max(state_variables$age))` years long, and attempting to predict ecological processes at an annual resolution from observations spaced by hundreds of years will be unreliable. So we need to chose a resolution for our predictions, i.e., we will estimate underlying ecological processes at a 50-year or 100-year temporal resolution. To do so, the data will be binned at a given resolution. The width of the bins is chosen according to the effect that binning will have state variables. Binning will aggregate some data (e.g., if two ovservations fall within the same bin they will be summed), and we want to keep as many observations as possible in independent bins. We want to interfere with the state variables as little as possible and maximise the number of observations over the time-series. 

A few summary checks can be useful here. We have done some data wrangling in long format, but the model requires wide data. We are going to pivot out data to wide and do some summaries. We are keeping the age and depth columns as identifiers to match with the charcoal data later.

```{r}
#| code-fold: false

# Pivot the data to wide format
state_variables_wide <- state_variables %>%
    pivot_wider(id_cols = c(age, depth), names_from = variablename, values_from = value) %>% 
    arrange(age)
```

```{r}
#| code-fold: false

# Time-span of the data divided by the numer of observations
max(state_variables_wide$age) / nrow(state_variables_wide)
```


```{r}
#| code-fold: false

# Age differences between successive observations
diff(state_variables_wide$age)
```


```{r}
#| code-fold: false

# The range of the age differences
range(diff(state_variables_wide$age))
```

```{r}
#| code-fold: false

# the average of the age differences
mean(diff(state_variables_wide$age))
```

```{r}
#| code-fold: false

# the standard deviation of the age differences
sd(diff(state_variables_wide$age))
```

This all looks great! If we bin the data at around 50-75 years, we will still have most of our data in independent bins.

::: {.callout-tip}

## Note: :scream:

The model can handle uneven gaps in time, which is great for palaeo-data. But if the time intervals in the data are too inconsistent, for example, consisting of very short intervals followed by very large intervals, then the model will struggle to fit. Inconsistent tinme intervals can occur in cores with very variable accumulation rates, and binning the data will result in summing many observations into one time bin, followed by a large gap in time.

:::


Now that we know roughly what bin-width to use to maximise the number of observations in the state variables, we will apply the same bin-width to both the state variables and the covariate data.

## Handling the covariates $X$

Covariates need to be handled according to decisions that were made around the state variables, that is, if the state variables were binned at a centurary-level resolution then the covariates must match that resolution. We _can_ have a finer temporal resolution of the covariates than we do of the state variables. In this dataset, our state variables have `r nrow(state_variables_wide)` observations, and our covariate has `r nrow(story_char_wide[story_char_wide$age <= max(state_variables_wide$age), ])` observations over the same time-span. Having more observations of environmental covariates is common in palaeo-data and works well with fitting the model.

::: {.callout-tip}

## Note

We can have a finer temporal resolution of the covariates, but we _cannot_ have a finer resolution of the state variables. If your data have a finer resolution of the state variables then it is possible to interpolate the covariate data to match. Interpolating the covariate data....
:::

Since we need to apply the same binning procedure to both the state variables and the covariate, I like to join both datasets. Charcoal was sampled to a greater depth than the pollen data, so we are going to clip the covariate to the extent of the state variables and join the two datasets by their common variable, age.

```{r}
#| code-fold: false
#| warning: false

# Clip covariate data to the extent of the state variables
# Join the two by age so that we get a square matrix
story_join <- story_char_wide %>%
  filter(age <= max(state_variables_wide$age)) %>%
  left_join(state_variables_wide)

# Always double check dimentions before/after joining!
# dim(story_join)
# tail(story_join)

```

Now we have all our data joined:

```{r}
#| code-fold: false
head(story_join, n = 10)
```

I always do lots of quick checks like `head()`, `tail()`, `dim()` (and many more!) to make sure to catch any coding mistakes!
```{r}
#| code-fold: false
tail(story_join, n = 10)
```

This all looks good, and we can now bin the data to an appropriate temporal resolution for fitting the model. Chosing a bin-width may take some trial and error, I'm going with a bin-width of 50 years.

```{r}
#| code-fold: false

# This code chunks the data into bins and gives us a grouping variable "bins"
bin_width <- 50
bins <- cut(story_join$age,
            breaks = seq(from = min(story_join$age),
            to = max(story_join$age + bin_width),
            by = bin_width), include.lowest = TRUE, labels = FALSE)
```

We now apply the bins to our data. For most types of covariates we want the _average_ value of the covariate in each bin. But for the pollen count data, we want the _sum_. This is because of the underlying multinomial distribution in the model.


```{r}
#| code-fold: false

# The following code 
story_binned <- bind_cols(bins = bins, story_join) %>%
  group_by(bins) %>% # Group the data by the bins so that we calculate per time bin
  summarise(
    age = mean(age, na.rm = T), # the center of the bin
    char_acc = mean(char_acc, na.rm = T), # mean charcoal accumulation rate per bin
    other = sum(other, na.rm = T), # the sums of the count data
    hardwood = sum(hardwood, na.rm = T),
    `Fagus grandifolia` = sum(`Fagus grandifolia`, na.rm = T),
    Ulmus = sum(Ulmus, na.rm = T),
    Quercus = sum(Quercus, na.rm = T)
  ) %>%
  arrange(desc(age))
# Be aware that the gaps in the pollen data are now filled with 0's not NA
```


::: {.callout-important}
## This is important :100:
We want the model to run from the past to the present, and the model runs from the top of the matrix to the bottom. So we want to make sure that the top row of the data is the _oldest_ date. Palaeo data are usually ordered trop the top of the core to the bottom (present to past), so we `arrange(desc(age))`.
:::

Let's see what the data look like binned at a 50 year resolution:

```{r}
head(story_binned, n = 10)
```

This is looking good, we have reduced the time-intervals over which to run the model. The data are now `r nrow(story_binned)` rows with complete covariate data, containing `r nrow(story_binned[which(rowSums(story_binned[ ,4:7]) != 0), ])` state variable observations. The original number of pollen observations was `r nrow(state_variables_wide)`, so we have not summed many observations within the same bins.

## Fitting `multinomialTS`

Ok, now we have some data organised into a site-by-species matrix $Y$, and a covariate matrix $X$.

### Finding initial conditions using `multinomialGLMM()`


- Fit without spp interactions
- Fit with spp interactions
  

### Fitting `multinomialTS()`

```{r}
# make sure data are going in the correct direction
```

### Interpreting outputs

There may be circumstances where coefficients change substantially by fitting the model in alternative ways, e.g., with or without interactions, of different covariates. We can use the post-hoc statistical tests like AIC and likelihood ratio tests to give some numerical support to one model or another. However, in the context of palaeoecological data at least, there is no known correct answer. We can only use the results to lend statistical support to multiple potential hypotheses. Palaeo-data lacks experimental manipulation and direct observation, so we suggest setting up multiple working hypotheses (i.e., @chamberlin1897), testing components of those hypotheses by fitting `multinomialTS` under different conditions, and finally assessing the statistical support lent to each hypotheses within the context of ecological knowledge.

### Comparing AIC


### Re-fitting if necessary

We are aiming to find the best optima possible from the model. Because this may not happen in the first attempt with initial conditions provided by `multinomialGLMM()`, we can refit the model with starting conditions from the output of `multinomialTS()`. This may initially sound like cheating, but remember the input values are only starting conditions. Starting with values that are closer to the 'true' optima is likely to provide a better and faster fit.


## To do

- [ ] attendance list
- [ ] session hand-out?
- [ ] feedback?
